{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + LlamaIndex Testing Notebook\n",
    "\n",
    "This notebook tests:\n",
    "- Ollama Python library\n",
    "- Downloading llama3.2:1b model (1.5B parameters)\n",
    "- Reading prompts from file\n",
    "- Context management\n",
    "- LlamaIndex integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ollama in /home/zigron/.local/lib/python3.10/site-packages (0.6.1)\n",
      "Requirement already satisfied: llama-index in /home/zigron/.local/lib/python3.10/site-packages (0.14.13)\n",
      "Requirement already satisfied: llama-index-llms-ollama in /home/zigron/.local/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: llama-index-embeddings-ollama in /home/zigron/.local/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/zigron/.local/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /home/zigron/.local/lib/python3.10/site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.9.4)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.6.13)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.13 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.14.13)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (0.5.6)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index) (3.9.2)\n",
      "Requirement already satisfied: pytest-asyncio>=0.23.8 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-embeddings-ollama) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zigron/.local/lib/python3.10/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zigron/.local/lib/python3.10/site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: certifi in /home/zigron/.local/lib/python3.10/site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /home/zigron/.local/lib/python3.10/site-packages (from httpx>=0.27->ollama) (3.7.1)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zigron/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.12.2)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.2.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (4.3.8)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (9.1.2)\n",
      "Requirement already satisfied: wrapt in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.17.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2025.3.0)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.0.8)\n",
      "Requirement already satisfied: numpy in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.23.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/lib/python3/dist-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (9.0.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.9.0)\n",
      "Requirement already satisfied: dataclasses-json in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.6.7)\n",
      "Requirement already satisfied: aiosqlite in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.22.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.12.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.6.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (6.0.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (80.10.1)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (3.4.2)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (4.15.0)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.0.46)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.2.18)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.31.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.3.0)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (2.14.0)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: pypdf<7,>=6.1.3 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.6.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.14.3)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/lib/python3/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<3,>=2.0.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.3.3)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zigron/.local/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2025.7.34)\n",
      "Requirement already satisfied: click in /home/zigron/.local/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/zigron/.local/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.5.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/zigron/.local/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/zigron/.local/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/zigron/.local/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: backports-asyncio-runner<2,>=1.1 in /home/zigron/.local/lib/python3.10/site-packages (from pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (1.2.0)\n",
      "Requirement already satisfied: pytest<10,>=8.2 in /home/zigron/.local/lib/python3.10/site-packages (from pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (9.0.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.20.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (6.6.4)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zigron/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.0.3)\n",
      "Requirement already satisfied: griffe in /home/zigron/.local/lib/python3.10/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /usr/lib/python3/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.3.1)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /home/zigron/.local/lib/python3.10/site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in /home/zigron/.local/lib/python3.10/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /home/zigron/.local/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/zigron/.local/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.7.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/zigron/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/zigron/.local/lib/python3.10/site-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zigron/.local/lib/python3.10/site-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/zigron/.local/lib/python3.10/site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (1.6.0)\n",
      "Requirement already satisfied: tomli>=1 in /home/zigron/.local/lib/python3.10/site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (2.3.0)\n",
      "Requirement already satisfied: packaging>=22 in /home/zigron/.local/lib/python3.10/site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (25.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/lib/python3/dist-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (2.11.2)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in /home/zigron/.local/lib/python3.10/site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (2.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zigron/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (2.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zigron/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.4.4)\n",
      "Requirement already satisfied: greenlet>=1 in /home/zigron/.local/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/zigron/.local/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/zigron/.local/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /home/zigron/.local/lib/python3.10/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/lib/python3/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install ollama llama-index llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Folder Structure and Sample Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created folder: ollama_test\n",
      "‚úì Created: prompt.txt\n",
      "‚úì Created: context.txt\n"
     ]
    }
   ],
   "source": [
    "# Create dedicated model folder\n",
    "model_folder = Path(\"./ollama_test\")\n",
    "model_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Create prompt.txt file\n",
    "prompt_file = model_folder / \"prompt.txt\"\n",
    "with open(prompt_file, \"w\") as f:\n",
    "    f.write(\"\"\"You are a helpful AI assistant. Answer questions concisely and accurately.\n",
    "Context: You are helping a developer test an agentic chatbot system with model switching capabilities.\n",
    "Task: Answer the user's questions while being aware of the context provided.\"\"\")\n",
    "\n",
    "# Create context.txt file\n",
    "context_file = model_folder / \"context.txt\"\n",
    "with open(context_file, \"w\") as f:\n",
    "    f.write(\"\"\"Project Context:\n",
    "- Building an agentic offline chatbot\n",
    "- Using FastAPI backend with React frontend\n",
    "- PostgreSQL for storing conversations\n",
    "- Model switching capability between different LLMs\n",
    "- Using Ollama for running local models\"\"\")\n",
    "\n",
    "print(f\"‚úì Created folder: {model_folder}\")\n",
    "print(f\"‚úì Created: prompt.txt\")\n",
    "print(f\"‚úì Created: context.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Llama 1.5B Model (llama3.2:1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ollama is running!\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running\n",
    "try:\n",
    "    ollama.list()\n",
    "    print(\"‚úì Ollama is running!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Ollama is not running. Please start Ollama first.\")\n",
    "    print(\"Run: ollama serve\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading llama3.2:1b... This may take a few minutes.\n",
      "‚úì Model llama3.2:1b downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download llama3.2:1b model (1.5B parameters)\n",
    "model_name = \"llama3.2:1b\"\n",
    "\n",
    "print(f\"Downloading {model_name}... This may take a few minutes.\")\n",
    "try:\n",
    "    # Pull the model\n",
    "    ollama.pull(model_name)\n",
    "    print(f\"‚úì Model {model_name} downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading model: {e}\")\n",
    "    print(\"\\nAlternative models you can try:\")\n",
    "    print(\"- llama3.2:1b (1.5B params)\")\n",
    "    print(\"- llama3.2:3b (3B params)\")\n",
    "    print(\"- phi3:mini (3.8B params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Available models:\n",
      "  - llama3.2:1b (Size: 1.23 GB)\n"
     ]
    }
   ],
   "source": [
    "# List available models (FIXED)\n",
    "models = ollama.list()\n",
    "print(\"\\nüìã Available models:\")\n",
    "\n",
    "# Handle different response structures\n",
    "if hasattr(models, 'models'):\n",
    "    model_list = models.models if hasattr(models.models, '__iter__') else models['models']\n",
    "else:\n",
    "    model_list = models.get('models', [])\n",
    "\n",
    "for model in model_list:\n",
    "    # Try different possible attribute names\n",
    "    model_name_attr = getattr(model, 'model', None) or getattr(model, 'name', None)\n",
    "    model_size = getattr(model, 'size', 0)\n",
    "    \n",
    "    if model_name_attr:\n",
    "        size_gb = model_size / (1024**3) if model_size > 0 else 0\n",
    "        print(f\"  - {model_name_attr} (Size: {size_gb:.2f} GB)\")\n",
    "    else:\n",
    "        # Fallback: just print the model object\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Basic Ollama Usage with Prompt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "You are a helpful AI assistant. Answer questions concisely and accurately.\n",
      "Context: You are helping a developer test an agentic chatbot system with model switching capabilities.\n",
      "Task: Answer the user's questions while being aware of the context provided.\n",
      "\n",
      "Context:\n",
      "Project Context:\n",
      "- Building an agentic offline chatbot\n",
      "- Using FastAPI backend with React frontend\n",
      "- PostgreSQL for storing conversations\n",
      "- Model switching capability between different LLMs\n",
      "- Using Ollama for running local models\n"
     ]
    }
   ],
   "source": [
    "# Read prompt from file\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "# Read context from file\n",
    "with open(context_file, \"r\") as f:\n",
    "    context = f.read()\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(system_prompt)\n",
    "print(\"\\nContext:\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Testing Basic Ollama Chat...\n",
      "\n",
      "Response: Based on the context provided, it appears that you're developing an agentic (self-aware) offline chatbot. This type of chatbot is designed to understand and respond to natural language inputs, often in a conversational manner.\n",
      "\n",
      "The fact that you're using FastAPI as your backend API and React as the frontend suggests a modern and user-friendly interface. The PostgreSQL database for storing conversations implies a robust and scalable data management system.\n",
      "\n",
      "Model switching capability between different LLMs (Large Language Models) is also an interesting aspect, which further supports the agentic nature of your chatbot. This ability to switch models in response to different conversation topics or user queries will allow you to adapt to various situations and improve the overall experience for users.\n",
      "\n",
      "Lastly, using Ollama for running local models on a Raspberry Pi (or another device) adds an extra layer of customizability, allowing you to fine-tune and experiment with different LLMs without relying on cloud-based services.\n"
     ]
    }
   ],
   "source": [
    "# Test basic chat with context\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': system_prompt\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f\"Context: {context}\\n\\nQuestion: What kind of chatbot are we building?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nü§ñ Testing Basic Ollama Chat...\\n\")\n",
    "response = ollama.chat(model=model_name, messages=messages)\n",
    "print(f\"Response: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Testing Streaming Response...\n",
      "\n",
      "FastAPI is a modern, fast (high-performance), web framework written in Python that allows developers to build APIs quickly and efficiently.\n",
      "\n",
      "As for your question about the type of chatbot, you're thinking of creating an agentic offline chatbot, which means using a conversational AI system that can engage with users over an extended period without requiring internet connectivity, such as a voice-controlled or smart home device.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test streaming\n",
    "print(\"\\nü§ñ Testing Streaming Response...\\n\")\n",
    "messages.append({\n",
    "    'role': 'user',\n",
    "    'content': 'Explain what FastAPI is in one sentence.'\n",
    "})\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    content = chunk['message']['content']\n",
    "    print(content, end='', flush=True)\n",
    "    full_response += content\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LlamaIndex Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LlamaIndex configured with Ollama\n"
     ]
    }
   ],
   "source": [
    "# Configure LlamaIndex with Ollama\n",
    "llm = Ollama(model=model_name, request_timeout=120.0)\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")  # Smaller embedding model\n",
    "\n",
    "# Set as default\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(\"‚úì LlamaIndex configured with Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embedding model ready\n"
     ]
    }
   ],
   "source": [
    "# Download embedding model if needed\n",
    "try:\n",
    "    ollama.pull(\"nomic-embed-text\")\n",
    "    print(\"‚úì Embedding model ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chat engine created with memory\n"
     ]
    }
   ],
   "source": [
    "# Create a simple chat engine with memory\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(\n",
    "    llm=llm,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"‚úì Chat engine created with memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Testing LlamaIndex Chat Engine...\n",
      "\n",
      "Q1: What database are we using?\n",
      "A1: We're using PostgreSQL as the relational database to store our conversations.\n",
      "\n",
      "Q2: What frontend framework did I mention?\n",
      "A2: You mentioned React as the frontend framework, specifically for building a chatbot with FastAPI backend and PostgreSQL database.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test chat with context memory\n",
    "print(\"\\nü§ñ Testing LlamaIndex Chat Engine...\\n\")\n",
    "\n",
    "# First message\n",
    "response1 = chat_engine.chat(f\"Context: {context}\\n\\nWhat database are we using?\")\n",
    "print(f\"Q1: What database are we using?\")\n",
    "print(f\"A1: {response1.response}\\n\")\n",
    "\n",
    "# Second message (should remember context)\n",
    "response2 = chat_engine.chat(\"What frontend framework did I mention?\")\n",
    "print(f\"Q2: What frontend framework did I mention?\")\n",
    "print(f\"A2: {response2.response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Document Indexing with LlamaIndex (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sample documents created\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents for RAG\n",
    "docs_folder = model_folder / \"docs\"\n",
    "docs_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample documents\n",
    "with open(docs_folder / \"fastapi_info.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"FastAPI is a modern, fast web framework for building APIs with Python.\n",
    "It supports async/await, automatic API documentation, and type hints.\n",
    "FastAPI is built on Starlette and Pydantic.\"\"\")\n",
    "\n",
    "with open(docs_folder / \"ollama_info.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"Ollama is a tool for running large language models locally.\n",
    "It supports models like Llama, Mistral, and many others.\n",
    "Ollama provides a simple API and command-line interface.\"\"\")\n",
    "\n",
    "print(\"‚úì Sample documents created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 12:49:38,442 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Indexed 2 documents\n"
     ]
    }
   ],
   "source": [
    "# Index documents\n",
    "print(\"\\nüìö Indexing documents...\")\n",
    "documents = SimpleDirectoryReader(str(docs_folder)).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(f\"‚úì Indexed {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing RAG Query...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 12:49:54,019 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-22 12:50:12,187 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is FastAPI built on?\n",
      "Answer: FastAPI is built using the following frameworks and libraries:\n",
      "\n",
      "1. Starlette\n",
      "2. Pydantic\n"
     ]
    }
   ],
   "source": [
    "# Query the index (RAG)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "print(\"\\nüîç Testing RAG Query...\\n\")\n",
    "response = query_engine.query(\"What is FastAPI built on?\")\n",
    "print(f\"Question: What is FastAPI built on?\")\n",
    "print(f\"Answer: {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Switching Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model switcher initialized\n"
     ]
    }
   ],
   "source": [
    "# Simulate model switching with context preservation\n",
    "class ModelSwitcher:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.current_model = model_name\n",
    "    \n",
    "    def chat(self, user_message, model=None):\n",
    "        if model and model != self.current_model:\n",
    "            print(f\"\\nüîÑ Switching from {self.current_model} to {model}\")\n",
    "            self.current_model = model\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': user_message\n",
    "        })\n",
    "        \n",
    "        # Get response\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt}\n",
    "        ] + self.conversation_history\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.current_model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        assistant_message = response['message']['content']\n",
    "        self.conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def get_context(self):\n",
    "        return self.conversation_history\n",
    "\n",
    "# Test model switcher\n",
    "switcher = ModelSwitcher()\n",
    "print(\"‚úì Model switcher initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Testing Model Switching with Context...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 12:50:23,707 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:1b\n",
      "User: My name is John\n",
      "Assistant: Hello John, I'm here to help you with your chatbot system. How can I assist you today? Are you experiencing any issues or need help with anything specific regarding your agentic chatbot system?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 12:50:29,202 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:1b\n",
      "User: What's my name?\n",
      "Assistant: Your name is John. How can I help you further, John? Is there a question or concern about your chatbot system that you'd like to discuss?\n",
      "\n",
      "\n",
      "üìù Conversation History:\n",
      "  user: My name is John...\n",
      "  assistant: Hello John, I'm here to help you with your chatbot...\n",
      "  user: What's my name?...\n",
      "  assistant: Your name is John. How can I help you further, Joh...\n"
     ]
    }
   ],
   "source": [
    "# Test conversation with model switching\n",
    "print(\"\\nüí¨ Testing Model Switching with Context...\\n\")\n",
    "\n",
    "response1 = switcher.chat(\"My name is John\")\n",
    "print(f\"Model: {switcher.current_model}\")\n",
    "print(f\"User: My name is John\")\n",
    "print(f\"Assistant: {response1}\\n\")\n",
    "\n",
    "response2 = switcher.chat(\"What's my name?\")  # Should remember\n",
    "print(f\"Model: {switcher.current_model}\")\n",
    "print(f\"User: What's my name?\")\n",
    "print(f\"Assistant: {response2}\\n\")\n",
    "\n",
    "# Show context is preserved\n",
    "print(\"\\nüìù Conversation History:\")\n",
    "for msg in switcher.get_context():\n",
    "    print(f\"  {msg['role']}: {msg['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Testing Complete!\n",
      "\n",
      "What we tested:\n",
      "1. ‚úì Ollama Python library\n",
      "2. ‚úì Downloaded llama3.2:1b model\n",
      "3. ‚úì Read prompts from file\n",
      "4. ‚úì Context management\n",
      "5. ‚úì Streaming responses\n",
      "6. ‚úì LlamaIndex integration\n",
      "7. ‚úì RAG with document indexing\n",
      "8. ‚úì Model switching with context preservation\n",
      "\n",
      "Next Steps for FastAPI Integration:\n",
      "1. Create FastAPI endpoints for chat\n",
      "2. Add WebSocket for streaming\n",
      "3. Integrate PostgreSQL for persistence\n",
      "4. Add conversation management\n",
      "5. Implement agent tools/functions\n",
      "6. Build React frontend\n",
      "\n",
      "Files created:\n",
      "- ./ollama_test/prompt.txt\n",
      "- ./ollama_test/context.txt\n",
      "- ./ollama_test/docs/fastapi_info.txt\n",
      "- ./ollama_test/docs/ollama_info.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚úÖ Testing Complete!\n",
    "\n",
    "What we tested:\n",
    "1. ‚úì Ollama Python library\n",
    "2. ‚úì Downloaded llama3.2:1b model\n",
    "3. ‚úì Read prompts from file\n",
    "4. ‚úì Context management\n",
    "5. ‚úì Streaming responses\n",
    "6. ‚úì LlamaIndex integration\n",
    "7. ‚úì RAG with document indexing\n",
    "8. ‚úì Model switching with context preservation\n",
    "\n",
    "Next Steps for FastAPI Integration:\n",
    "1. Create FastAPI endpoints for chat\n",
    "2. Add WebSocket for streaming\n",
    "3. Integrate PostgreSQL for persistence\n",
    "4. Add conversation management\n",
    "5. Implement agent tools/functions\n",
    "6. Build React frontend\n",
    "\n",
    "Files created:\n",
    "- ./ollama_test/prompt.txt\n",
    "- ./ollama_test/context.txt\n",
    "- ./ollama_test/docs/fastapi_info.txt\n",
    "- ./ollama_test/docs/ollama_info.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
